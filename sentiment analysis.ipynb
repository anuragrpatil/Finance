{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinance import Pinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"AAPL\"\n",
    "\n",
    "stock = Pinance(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = stock.get_news() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 11:54:20.648908 4583550400 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "I0519 11:54:25.644062 4583550400 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import transformers as ppb # pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 11:54:29.965715 4583550400 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/anurag/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I0519 11:54:29.967367 4583550400 modeling.py:588] extracting archive file /Users/anurag/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/_p/9dls8ssn77z2q_b4r1c5pbhh0000gn/T/tmp6seb_t0t\n",
      "I0519 11:54:35.013293 4583550400 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DistillBert here to get a quick baseline\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# model = model_class.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 11:55:01.059229 4583550400 tokenization.py:190] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/anurag/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def dataProcessingEmbedding(Dataset):\n",
    "  tokenized_text = []\n",
    "  indexed_tokens = []\n",
    "  segments_ids = []\n",
    "  marked_text_lis = []\n",
    "  error_lis = []\n",
    "  for i in range(0,len(Dataset)):\n",
    "\n",
    "    marked_text = \"[CLS] \"+str(Dataset)+\" [SEP]\" # a window of previous 20 words. For the first 20 words isn't as much words as possible. hence the very last first one is just a single word. \n",
    "\n",
    "    try:\n",
    "      # Tokenize our sentence with the BERT tokenizer.\n",
    "        tokens = tokenizer.tokenize(marked_text)\n",
    "        marked_text_lis.append(marked_text)\n",
    "        tokenized_text.append(tokens)\n",
    "        indexed_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "        segments_ids.append([1] * len(tokens))\n",
    "    except:\n",
    "        error_lis.append(i)\n",
    "\n",
    "  return tokenized_text, indexed_tokens, segments_ids, marked_text_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, indexed_tokens, segments_ids, marked_text_lis = dataProcessingEmbedding(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "def convertTokens_Tensors(indexed_tokens,segments_ids):\n",
    "  pad = len(max(indexed_tokens, key=len))\n",
    "  indexed_tokens_pad = np.array([i + [0]*(pad-len(i)) for i in indexed_tokens])\n",
    "  segments_ids_pad = np.array([i + [1]*(pad-len(i)) for i in segments_ids])\n",
    "\n",
    "  tokens_tensor = torch.Tensor(indexed_tokens_pad)\n",
    "  segments_tensors = torch.tensor(segments_ids_pad)\n",
    "\n",
    "  return tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor, segments_tensors = convertTokens_Tensors(indexed_tokens,segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  encoded_layers,_ = model(tokens_tensor.to(torch.int64), attention_mask=segments_tensors.to(torch.int64))\n",
    "  token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "  semanticseqs = torch.mean(torch.sum(token_embeddings[-4:,:,:,:], dim=0),dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
